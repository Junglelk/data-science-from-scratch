# 数据科学入门 *Data Science from Scratch*

第一章是大致简介了一下相关的工作内容，第二章是需要的python入门知识，第三章也就是我创建这个项目的这一章是数据可视化，介绍了一些数据可视化的内容。

## 数据可视化 *data-visualization*

简而言之就是基本见识一下matplotlib包，包括：

* 线图
* 条形图
* 散点图

更深入的，没涉及。

## 线性代数 *linear algebra*

## 梯度下降分析法 *gradient descent*
梯度下降分析法。简而言之就是顺着偏导数（导数）方向上，单步增加或减少以达到最优解的技术。 这次写了一个例子，算是搞明白了一点点，但不理解的是为什么要用步长与在
该点的导数值获取到用于更新参数的步长数组，不知道是梯度下降分析法就该这么乘得步长数组，还是仅仅适用于平方和函数。但确实得到了近似最优解。

## k最近邻法 *k-nearest neighbors*
不难看出思想很简单，但“距离”这个概念不适合相对较高的维度，如果要在高维度使用最近邻法，势必要在一个合适的平面做降维，这可能带来额外的问题...  
题外话是，想起一个笑话：如果你打算用正则解决一个问题，那么你就拥有了两个问题...  
当成本过高时，选择换一个方案似乎是更好的选择。

## 简单线性回归 *simple linear regression*
其实和上一章中间隔了一个朴素贝叶斯，但有朋友推荐我先看简单线性回归，因为这个朋友很专业，所以我听她的。  
这里用到了最小二乘法，但我并没有系统学习过相关的内容，所以去找了一下，[最小二乘法的本质](https://www.zhihu.com/question/37031188)，这个回答下的高赞用比较直观的方式讲了一下，
但感觉离学会还有点距离。  
练习了两天最小二乘法，对线性回归的最小二乘法有了点了解，做点简单的题目是没问题了。  
本章举的例子是线性回归，使用最小二乘法和梯度下降法都进行了计算，结果是一致的。那么就有一个问题：最小二乘法和梯度下降法的适用场景是什么？  
不难看出计算量上面显然的是最小二乘法相对更小，甚至可以说一次计算即可，但梯度下降法计算量极大。但随着维度的增加，最小二乘法的计算也趋向复杂、繁琐，而梯度下降法却没什么变化。  
所以简单分析一下的话，如上就是原因？

## 所有之外的话
如同我大学一战考研失败后想通的一点：既然想转计算机，为什么不直接考计算机的研究生呢？现在我也感受到差不多的想法了。烦心事似乎一直没有离我而去，真希望我能睡一个好觉。  
项目最终还是加起班来了，说好说坏？好歹我写上代码了，已经一个月工作没写代码了...  
最近除了加班还是加班，疫情、工作烦心，力扣也没咋刷了，数据科学也没咋学，真是慌慌张张，匆匆忙忙，好像有很多琐碎的事横亘在想法中央。
要说今天想干嘛？我今天不想学习
加班，不想学习  
勉强写完了统计假设检验的一个例子，说实话没咋看明白，后面看看原版是怎么写的吧。最近都是加班加班加班，严重怀疑那些996的是真的假的，这么煎熬
的生活感觉就算是给钱也熬不下来。
烦躁，虽然工作不顺，但这次是我自己的错，很难受。今天排查了一些业务逻辑的bug，又转过去接手了一个烂摊子，一个东拼西凑，最后勉强凑出来的烂摊子...烦心  
今天可能要加一个过12点的班 <- 实际上加班到11点42，莫名其妙感觉自己赚了  
纸上练习最小二乘法，感觉好累，好久没做过这些演算了...  
最近两天吃完晚饭都巨困，困意袭来翻江倒海那种，完全扛不住，只能暂时睡一会儿再起来洗澡，不知道是不是白天文档写多了太耗脑细胞了。今天回顾看了一下之前的多元分析，不知道
是因为太累还是怎么，看不下去，完全看不下去，硬看也看不懂，那就歇着吧。  
去看了线程装饰器相关的内容，打算写一篇博客，当然这是明天的事儿了
